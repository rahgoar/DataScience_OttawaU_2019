# -*- coding: utf-8 -*-
"""R2_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MvCvBztl5kdtRaYcuh1r81cNmFk48myE

Key R packages for text classification and NLP include:
tidytext: This package, part of the Tidyverse, provides a "tidy" approach to text mining, making it easy to convert text data into a format suitable for analysis with other Tidyverse tools. It's particularly useful for tasks like sentiment analysis and topic modeling.

quanteda: A powerful and efficient package for quantitative text analysis, including features like tokenization, n-gram creation, document-feature matrix construction, and various text classification models.

tm (Text Mining): A foundational package for text mining in R, offering tools for text preprocessing, corpus management, and various text analysis techniques.
sentimentr: Specifically designed for sentiment analysis, allowing for the calculation of sentiment scores at various levels (e.g., sentence, document).

textrecipes: Integrates text preprocessing steps into the tidymodels framework, making it easier to build and evaluate text classification models.

Machine Learning Packages: R's general machine learning packages, such as caret or tidymodels, can be used in conjunction with the text analysis packages to build and train text classification models using algorithms like Naive Bayes, Support Vector Machines (SVMs), or tree-based methods.

Tidymodels Approach Example
The tidymodels framework is built around the tidyverse principles, providing a modular and consistent way to build machine learning pipelines.
Load libraries:
Code

    library(tidymodels)
    library(dplyr)
Load & prepare text data:
Use a dataset with text and a classification target.
Convert text into features using a recipe. For example, you might use step_tokenize to split text into words and then step_tfidf to create a TF-IDF representation.
Code

    # Example: Convert text into TF-IDF features
    my_recipe <- recipe(target_variable ~ ., data = text_data) %>%
      step_tokenize(text_column) %>% # Tokenize the text column
      step_stopwords(text_column) %>% # Remove stop words
      step_tfidf(text_column) # Compute TF-IDF features
Define a model:
Use the parsnip package to choose a classification model (e.g., logistic regression, support vector machine).
Code

    # Example: Define a logistic regression model
    log_reg_model <- logistic_reg() %>%
      set_engine("glm") %>%
      set_mode("classification")
Create a workflow:
Combine the recipe and the model into a single workflow object.
Code

    # Combine the recipe and model
    my_workflow <- workflow() %>%
      add_recipe(my_recipe) %>%
      add_model(log_reg_model)
Fit the workflow:
Fit the workflow to your training data using fit().
Code

    # Fit the workflow
    my_fit <- my_workflow %>%
      fit(data = training_data)
Make predictions:
Use the fitted object to make predictions on new data using predict().
Caret Approach Example
Caret is an older, monolithic package that bundles many functions for predictive modeling.
Load caret:
Code

    library(caret)
Prepare text data:
Pre-processing in caret is often done directly on the data frame.
Train the model:
Use the train() function to train a classification model.
Code

    # Example: Train a model with caret
    model_fit <- train(
      target_variable ~ .,
      data = training_data,
      method = "glm", # Specify the model engine
      preProcess = c("center", "scale", "nzv"), # Pre-processing steps
      trControl = trainControl(method = "cv", number = 10) # Cross-validation
    )
When to Choose Which
Choose tidymodels for: New projects, complex pipelines, and if you prefer the modular and pipeable syntax of the tidyverse.
Choose caret for: Existing projects, or when you need a quick, all-in-one solution and don't need the flexibility of tidymodels.
"""

